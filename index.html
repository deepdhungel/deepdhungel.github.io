
<!DOCTYPE html>
<html>
  <head>
    <title>Curriculum Vitae</title>
	<link href="
https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.min.css
" rel="stylesheet">
	<style>
     /* Add some styling for the CV */
      body {
        font-family: Futura PT;
        margin: 0;
        padding: 0;
      }
      .header {
        background-color: hsl(210.46deg 53.64% 42.54%); /* Dark blue Color */
        padding: 15px;
        text-align: center;
      }
      .header h1 {
        margin: 0;
      }
      .left-col {
        float: left;
        width: 100%;
		text-align:center;
        padding: 15px;
        background-color: hsl(210.46deg 74.62% 90.69%); /* Faded blue Color */
      }
      .right-col {
        float: right;
        width: 75%;
        padding: 15px;
      }
      .clear {
        clear: both;
      }
           .section-header {
        font-weight: bold;
        margin-top: 15px;
        text-decoration: underline;
      }

      .photo {
        float: right;
        width: 25%;
        padding: 15px;
      }
      .photo img {
        width: 100%;
        height: auto;
      }
	    h1#name {
    color: #FFFFFF; /* Microsoft blue color */
}
a{
color: black;
}
	.category-name {
	 color: hsl(210.46deg 69.96% 39.38%);
	}
	   
    </style>
  </head>
  <body>
  
 
    <div class="header">
      <h1 id="name">Deep Kiran Dhungel</h1>
    </div>
    <div class="left-col">
      <span> <i class="bi bi-telephone-fill"></i> +49  15 73 32 81 22 0 </span> &nbsp;&nbsp; &nbsp;&nbsp;
      <span><i class="bi bi-github"></i> <a href="https://github.com/deepdhungel">https://github.com/deepdhungel</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
      <span> <i class="bi bi-envelope-fill"></i> deep.dhungel1@gmail.com</span>&nbsp;&nbsp;&nbsp;&nbsp;
      <span><i class="bi bi-award-fill"></i> <a href="https://www.youracclaim.com/users/deepdhungel">https://www.youracclaim.com/users/deepdhungel</a></span>
    </div>
  <div class="photo" style="width:250px">
      <img src="prfpic.png" alt="Profile Picture">
  </div>
 <div class="right-col">
      <h1 align="left" class="category-name">Key Skills and Competencies</h1>
      <ul style="list-style-position: inside;">
        <li>Extensive knowledge in database design.</li>
        <li>ETL design, implementation and maintenance in production.</li>
		<li align="left"> Extensive knowledge in database design, ETL design, implementation and maintenance in production.
</li> 
<li align="left"> Expertise in RDBMS such as PostgreSQL, MSSQL and MySQL.</li> 
<li align="left"> Familiarity of ETL tools such as Talend, Azure Data Factory and Apache Airflow.
</li> 
<li align="left"> Proficiency with Python and related libraries such as Pandas, Numpy, Scipy.
</li> 
<li align="left"> Competence in R and its libraries such as dplyr, dbplyr, lubridate, stringr, tidyr.
</li> 
<li align="left"> Acquaintance with visualization and reporting with Tableau, PowerBI including complex calculated fields.</li> 
<li align="left"> Command in AWS Cloud, Oracle Cloud, Azure Cloud and their arrays of tools and best practices.</li> 
<li align="left"> First experiences with Jenkins, Kubernetes and containerization with Docker.
</li> 
<li align="left"> Extensive knowledge in Unit-testing and Software Engineering best practices.</li> 
      </ul>
		<br>
<h1 align="left" class="category-name">Professional Experience</h1>
<h2 align="left">Senior Data Manager</h2>
<h2 align="left">GVL - Gesellschaft zur Verwertung von Leistungsschutzrechten mbH </h2>
<p1 align="left">(May 2021 – Present)</p1>
<br><br>
<ul>	 
   <li> Successfully manage multiple projects for the Digital Data Exchange (DDEX RDR 1.4) standard ETL pipelines, ensuring technical communication and partnership with B2B clients such as Universal Music Group, Sony Music Entertainment, Warner Music Group, and Absolute Music Group.</li>
   <li> Oversee end-to-end projects for the upgrade of legacy Digital Data Exchange 1.3.1 (DDEX MLC 1.3.1) standard ingestion and processing pipelines to the latest Digital Data Exchange 1.4 (DDEX RDR 1.4) standard in Test, QA, and Production. Manage onboarding of existing and new partners to DDEX 1.4. </li>
   <li> Lead cross-functional projects to integrate music product and usage data, video product and usage data, and video clip product and usage data from private and public radio and TV stations in Germany to the business data warehouse, ensuring optimal ETL routes.</li>
   <li> Identify and collect technical Data Engineering requirements from B2B partners (Major Record Labels) and collaborate with them to find the best possible solutions. Continuously improve existing data structures and data pipelines to enhance overall efficiency. </li>
   <li> Monitor and control the quality of incoming data from business partners to meet Datawarehouse requirements. Proactively reclaim data from partners if necessary to ensure the accuracy and completeness of data. </li>
   <li> Find solutions to constantly improve existing data structures and data pipelines. </li> 
   <li> Successfully manage projects for the migration of legacy Data in MongoDB to the Apache Drill and Snowflake. Analyze existing and new data in Datawarehouse and provide insights to boost Data Science Projects. </li>

		 <br>
		<li2 align="left"> <strong>Environment:</strong> 

PostgreSQL, MongoDB, Mongocharts, Swagger UI, Talend Openstudio, Tableau, Grafana, Python, Pandas, Anaconda, Kibana, Java Microservices, Snowflake, Apache Drill, RabbitMQ</li2>
</ul>
	<br>
<br>
<h2 align="left">Data Engineer</h2>
<h2 align="left">FitX Digital GmbH  </h2>
<p1 align="left">(October 2020 – February 2021)</p1>
<br><br>
<ul>	
   <li> Designed and architected Fact and Dimension tables of Star Schema for Data Warehousing, resulting in streamlined and efficient data processing.</li>
   <li> Developed ETL (Extract, Transform and Load) pipelines to integrate data from the Data Lake to the Data Warehouse, resulting in optimized data flow and increased data quality.</li>
   <li> Delivered robust data for visualization, ensuring that the data was accurate, complete, and readily available for analysis and reporting.</li>
   <li> Managed version control for the database and schema migration at different instances (Development, QA, and Production), ensuring seamless and error-free deployment of database changes. </li>
   <li> Successfully managed release pipelines at Development, QA, and Production, ensuring that changes were properly tested and deployed.</li>
   <li> Find solutions to constantly improve existing data structures and data pipelines. </li> 
   <li> Effectively managed Azure Data Factory component and component codes at the repository, ensuring that the code was properly documented and version-controlled for efficient management of the platform. </li>
		
		<br>
		<li2 align="left"> <strong>Environment: </strong>

Azure Data Factory, Power BI, Active Directory, Redgate Flyway, Azure Repos, Git, MySQL, AWS S3</li2>
</ul>
	<br>

<h2 align="left">Data Engineer</h2>
<h2 align="left">YAS.life</h2>
<p1 align="left">(April 2019 – September 2020)</p1>
<br><br>
<ul>	 
<li> Designed and implemented complex ETL pipelines, using directed acyclic graphs (DAGs) and task-dependencies, to streamline data processing and improve data quality.</li>
<li> Integrated data from multiple channels, including product, marketing, and production backend databases, into the staging analytics RDBMS and Data Warehouse, enabling more comprehensive analysis and reporting.</li>
<li> Optimized the performance of business-critical queries, resolving issues to ensure the efficient processing of data and timely delivery of insights.</li>
<li> Designed, implemented, and maintained a platform to provide ad-hoc access to large data sets, enabling team members to more easily access and analyze data. </li>
<li> Conducted statistical analyses of health-based data, including descriptive statistics, regression model analysis, significance testing, standard deviation, and sample size determination, and hypothesis testing, providing valuable insights into trends and patterns in the data. </li>
<li> Prepared visualizations and reports for B2B and B2C processes, effectively communicating insights to stakeholders and driving data-driven decision-making. </li>
<li> Managed Amazon Web Services instances, ensuring efficient and cost-effective use of cloud resources. Implemented continuous integration and deployment processes, ensuring that changes were properly tested and deployed in a timely and efficient manner.</li>
<li>Shared knowledge, organized workshops, and provided training to colleagues, fostering a culture of data-driven decision-making and continuous improvement. </li>
	
		<br>
		<li2 align="left"> <strong>Environment: </strong>

PostgreSQL, Python, Apache Airflow, R / R Openstudio, Tableau, AWS EC2, AWS RDS, AWS S3, Jenkins, Kubernetes, Git</li2>
</ul>
	<br>
      <br>

<h2 align="left">Data Engineer</h2>
<h2 align="left">kloeckner.i GmbH </h2>
<p1 align="left">(October 2017 – March 2019)</p1>
<br><br>
<ul>	
   <li> Leveraged automation to optimize several critical processes in Business Intelligence, including dynamic pricing, customer segmentation, and steel sales validation. </li>
   <li> Designed and implemented a comprehensive business data warehouse that integrates data from multiple sources and supports various reporting and visualization requirements. </li>
   <li> Developed a monitoring tool for ETL jobs, which enabled real-time tracking of the data flow and detection of issues. </li>
   <li> Built custom Java components and data models to support the ETL pipelines, resulting in significant efficiency gains and higher data quality.</li>
   <li>  Conducted thorough cross-validation of MySQL data with SAP BW, ensuring accuracy and consistency of key metrics such as sales, billing, and customer data. </li>
   <li> Demonstrated proficiency in database administration and troubleshooting, with a focus on MySQL management systems. </li>
   <li> Spearheaded the development of the Zendesk integration pipeline from scratch, streamlining data flows and enhancing the overall customer experience. </li>
		
		<br>
		<li2 align="left"> <strong>Environment: </strong>

Talend Openstudio, Tableau, MySQL, SAP BW, Zendesk, Automated Batchprocesses</li2>
</ul>
		<br>
<h2 align="left">Database Developer </h2>
<h2 align="left">Charité, Berlin</h2>
<p1 align="left">(March 2017 - July 2017)</p1>
<br>
<br>
<ul>	
   <li> Data pipeline development for drugs prescribed to Charité patients in Python and libraries (Pandas, Numpy, Scipy) for different targeted analyses and workflows, e.g. patient data segmentation. Utilized best practices in data engineering and machine learning to ensure high-quality and efficient code. Please check my Github profile for further information and code examples.</li>
   <li>  Designed, modeled, and normalized complex table structures for Data Warehousing, ensuring optimal performance and data integrity. Administered and optimized the MySQL and PostgreSQL installations to improve query performance and reduce downtime. </li>
   <li> Conducted data cleaning and preparation using advanced techniques and tools such as regular expressions, data imputation, and outlier detection. Developed automated data validation and quality control checks to ensure the accuracy and completeness of the data.</li>
   <li>  Conducted extensive data mining and ad-hoc statistical analyses of patient and drug data, leveraging techniques such as regression, clustering, and dimensionality reduction. Created insightful visualizations and graphical analyses to communicate findings to stakeholders. </li>
   <li> Conducted different tests for non-normal patient data to predict the stochastic equivalence as well as significant differences, null and alternative hypotheses tests. Utilized statistical software such as R or Python, as well as tools such as Excel, to analyze and visualize the data.
</li>
	 
		<br>
		<li2 align="left"> <strong>Environment:</strong> Python, MySQL, PostgreSQL</li2>
</ul>
<br>
<h1 align="left" class="category-name">Academic Qualifications</h1>
<h2 align="left">Bioinformatics, Freie Universität Berlin</h2>
<p2 align="left">Thesis: Analysis of the Repurposing of Previously Withdrawn Drugs.</p2><br>
<p1 align="left">(Graduation Year 2017)</p1>
<br>
<h1 align="left" class="category-name">Training and Certifications</h1>
<h2 align="left">Oracle Autonomous Database Cloud 2019 Certified Specialist</h2>
<p2 align="left">Institution: Oracle</p2><br>
<p1 align="left">Issued On: April 2020</p1><br>
<p1 align="left">Valid Till: April 2021</p1><br>
<p1 align="left">Certification ID: 274384621OADB19-F</p1><br>
<p1 align="left">Validation Link:<a href="https://www.credly.com/badges/475a047f-f9bf-42ab-b2eb-2d756e6153ed">https://www.credly.com/badges/475a047f-f9bf-42ab-b2eb-2d756e6153ed</a></p1>
<br>
<h2 align="left">Oracle Cloud Infrastructure 2019 Certified Architect Associate</h2>
<p2 align="left">Institution: Oracle</p2><br>
<p1 align="left">Issued On: April 2020</p1><br>
<p1 align="left">Valid Till: April 2021</p1><br>
<p1 align="left">Certification ID: 274540788OCSIAAS2019-F</p1><br>
<p1 align="left">Validation Link:<a href="https://www.credly.com/badges/b270d17c-6ecc-4127-b021-9146166b05ca">https://www.credly.com/badges/b270d17c-6ecc-4127-b021-9146166b05ca</a></p1>
<br>
<h2 align="left">Oracle Cloud Infrastructure Foundations 2020 Associate</h2>
<p2 align="left">Institution: Oracle</p2><br>
<p1 align="left">Issued On: April 2020</p1><br>
<p1 align="left">Valid Till: April 2021</p1><br>
<p1 align="left">Certification ID: 274384621OCIBF2020-F</p1><br>
<p1 align="left">Validation Link:<a href="https://www.credly.com/badges/3874642c-1119-46df-ae3a-423220ed3ff0">https://www.credly.com/badges/3874642c-1119-46df-ae3a-423220ed3ff0</a></p1>
<br>
<h2 align="left">AWS Certified Developer - Associate</h2>
<p2 align="left">Institution: Amazon Web Services (AWS)</p2><br>
<p1 align="left">Issued On: April 2020</p1><br>
<p1 align="left">Valid Till: April 2023</p1><br>
<p1 align="left">Certification ID: E2SJSFL2M1BQQ5K3</p1><br>
<p1 align="left">Validation Link:<a href="https://www.credly.com/badges/2097f13a-fae7-421f-863e-7cccce4e4cc5">https://www.credly.com/badges/2097f13a-fae7-421f-863e-7cccce4e4cc5</a></p1>
<br>
<h2 align="left">AWS Certified Cloud Practitioner</h2>
<p2 align="left">Institution: Amazon Web Services (AWS)</p2><br>
<p1 align="left">Issued On: March 2020</p1><br>
<p1 align="left">Valid Till: March 2023</p1><br>
<p1 align="left">Certification ID: HQB0FCNKGM1Q11CP</p1><br>
<p1 align="left">Validation Link:<a href="https://www.credly.com/badges/bce3b395-93b8-4628-8c02-0962100fa9a4">https://www.credly.com/badges/2097f13a-fae7-421f-863e-7cccce4e4cc5</a></p1>
<br>
<h2 align="left">Machine Learning</h2>
<p2 align="left">Institution: Stanford Online</p2><br>
<p1 align="left">Issued On: February 2020</p1><br>
<p1 align="left">Valid Till: Non-Expiry</p1><br>
<p1 align="left">Certification ID: NUJCKJ9AG9LB</p1><br>
<p1 align="left">Validation Link:<a href="https://www.coursera.org/account/accomplishments/verify/NUJCKJ9AG9LB">https://www.coursera.org/account/accomplishments/verify/NUJCKJ9AG9LB</a></p1>
</ul>
</body>
